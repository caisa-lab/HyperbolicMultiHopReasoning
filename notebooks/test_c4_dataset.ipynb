{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.util import load_c4_dataset\n",
    "from src.datasets import C4Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/t5-v1_1-base\"\n",
    "print(\"Loading Tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _span_corruption(text, corruption_rate=0.15, average_length_of_spans=3):\n",
    "    input_tokens = tokenizer.tokenize(text)\n",
    "    total_tokens = len(input_tokens)\n",
    "\n",
    "    total_corrupted_tokens = int(total_tokens * corruption_rate)\n",
    "    total_spans = total_corrupted_tokens // average_length_of_spans\n",
    "\n",
    "    span_lengths = np.random.poisson(average_length_of_spans, total_spans)\n",
    "    span_lengths = np.clip(span_lengths, 1, total_tokens // total_spans)\n",
    "\n",
    "    total_corrupted_tokens = span_lengths.sum()\n",
    "    if total_corrupted_tokens != int(total_tokens * corruption_rate):\n",
    "        difference = int(total_tokens * corruption_rate) - total_corrupted_tokens\n",
    "        if difference > 0:\n",
    "            for i, current_length in enumerate(span_lengths):\n",
    "                if current_length < (total_tokens // total_spans):\n",
    "                    span_lengths[i] += 1\n",
    "                    total_corrupted_tokens += 1\n",
    "                    if total_corrupted_tokens == int(total_tokens * corruption_rate):\n",
    "                        break\n",
    "        else:\n",
    "            for i, current_length in enumerate(span_lengths):\n",
    "                if current_length > 1:\n",
    "                    span_lengths[i] -= 1\n",
    "                    total_corrupted_tokens -= 1\n",
    "                    if total_corrupted_tokens == int(total_tokens * corruption_rate):\n",
    "                        break\n",
    "\n",
    "    span_starts = []\n",
    "    current_position = 0\n",
    "    for idx, length in enumerate(span_lengths):\n",
    "        if current_position >= total_tokens:\n",
    "            break\n",
    "        length = min(length, total_tokens - current_position)\n",
    "        start = np.random.randint(current_position, total_tokens - sum(span_lengths[idx:]) * 2 + 1)\n",
    "        span_starts.append(start)\n",
    "        current_position = start + length\n",
    "    span_starts.sort()\n",
    "\n",
    "    output_tokens = input_tokens.copy()\n",
    "    corrupted_tokens = []\n",
    "    sentinel_counter = 0\n",
    "    sum_lengths = 0\n",
    "    for start, length in zip(span_starts, span_lengths):\n",
    "        end = min(start + length, total_tokens)\n",
    "        sentinel_token = [f'<extra_id_{sentinel_counter}>']\n",
    "        corrupted_tokens.append(sentinel_token)\n",
    "        corrupted_tokens.extend(input_tokens[start:end])\n",
    "        output_tokens[start-sum_lengths:end-sum_lengths] = sentinel_token\n",
    "        sentinel_counter += 1\n",
    "        sum_lengths += length-1\n",
    "        \n",
    "    input_tokens = output_tokens\n",
    "    target_tokens = [token for sublist in corrupted_tokens for token in sublist]\n",
    "\n",
    "    input_sequence = tokenizer.convert_tokens_to_string(input_tokens)\n",
    "    target_sequence = tokenizer.convert_tokens_to_string(target_tokens)\n",
    "\n",
    "    return input_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def _prefix_language_modeling(text):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        split_point = random.randint(1, len(tokens)-1)\n",
    "        input_tokens = tokens[:split_point]\n",
    "        target_tokens = tokens[split_point:]\n",
    "        \n",
    "        input_sequence = tokenizer.convert_tokens_to_string(input_tokens)\n",
    "        target_sequence = tokenizer.convert_tokens_to_string(target_tokens)\n",
    "        return input_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The<extra_id_0>s over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\n",
      "<extra_id_0> quick brown fox jump\n"
     ]
    }
   ],
   "source": [
    "input_sequence, decoder_sequence= _span_corruption(\"The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\")\n",
    "print(input_sequence)\n",
    "print(decoder_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../c4/en/c4-train.{:05d}-of-01024.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataframe with c4 Data...: 100%|██████████| 1/1 [00:09<00:00,  9.89s/it]\n"
     ]
    }
   ],
   "source": [
    "list_of_texts = load_c4_dataset(base_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/t5-v1_1-base\"\n",
    "print(\"Loading Tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup Dataset: Remove texts with < 3 corrupted tokens: 100%|██████████| 356317/356317 [00:05<00:00, 63561.34it/s]\n",
      "Cleaned 3478 Datapoints remaining 352839 Datapoints\n"
     ]
    }
   ],
   "source": [
    "c4_dataset = C4Dataset(list_of_texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c4_dataloader_train = DataLoader(c4_dataset, batch_size = 32, shuffle=False)\n",
    "c4_iter = iter(c4_dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = next(c4_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target = first_batch[0], first_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(input, padding=True, truncation=True, return_tensors='pt')\n",
    "tokenized_labels = tokenizer(target, padding=True, truncation=True, return_tensors='pt')\n",
    "input_ids = tokenized_inputs['input_ids']\n",
    "attention_mask = tokenized_inputs['attention_mask']\n",
    "labels = tokenized_labels['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginners BBQ Class Taking Place in Missoula!\n",
      "Do you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\n",
      "He will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\n",
      "The cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.\n",
      "Beginners BBQ Class Taking Place in Missoula! Do you want to get better at making delicious BBQ? You will<extra_id_0> put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Ba<extra_id_1>onestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills. He will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques<extra_id_2>s, meat selection and trimming, plus smoker and fire information. The cost to be in the class is $35 per person<extra_id_3>s it is free. Included in the cost will be either <extra_id_4>t-shirt or<extra_id_5>a<extra_id_6> will be tasting samples of each<extra_id_7> prepared.</s>\n",
      "<extra_id_0> have the opportunity,<extra_id_1>lay from L<extra_id_2>, recipes, timeline<extra_id_3>, and for spectator<extra_id_4>a <extra_id_5> <extra_id_6>pron and you<extra_id_7> meat that is\n"
     ]
    }
   ],
   "source": [
    "print(list_of_texts[0])\n",
    "print(input[0])\n",
    "print(target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
